---
title: "Data_Processing"
author: "Susanne_Rehorst"
date: "23-5-2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load or install packages
```{r warning=FALSE}
library(readtext)
library(sylcount)
library(stringr)
library(tm)
library(NLP)
library(udpipe)
library(textclean)
library(dplyr)
```

Set working directory
```{r}
setwd("~/Thesis")
```

# Data Processing
This R Markdown contains the steps to load in the data and applying pre-processing steps, which are required for text analysis

## Loading in Data
For every ESG regulation, the text is loaded in from a .txt file. Subsequently, every regulation is stored as a string value in one cell. Every uploaded document will get an ID, which is used to link the raw text with meta data. First, texts are put into a character string
```{r warning=FALSE}
setwd("~/Thesis/Raw_Data_Text")
Raw_Data <- readtext::readtext("ID*.txt", encoding = "UTF-8")
Vector_Data <- Raw_Data$text
Doc_ID <- c(seq(1, length(Vector_Data), by=1))
```

A separate document with ESG key topics is loaded in
```{r}

```

The meta data is also loaded in. 
```{r}

```

## Merging Data
Mandatory E, S, or G regulations are merged if they come from the same sources and apply to the same region. 
```{r}

```

## Data Cleaning
Preliminary cleaning: Remove double white spaces, double quotes and line break (/n) expressions. This is recommended for the quantity and readability analysis in order to work properly. 
```{r}
## ----- General cleaning that is used for all analysis -----

# Remove line break expressions: 
Vector_Data <- gsub("\r?\n|\r"," ", Vector_Data)
# Remove double quotation marks, comma's, parentheses, special characters and bullet points
Vector_Data <- gsub('[“”"(),•%$€@/#]', '', Vector_Data)
Vector_Data <- gsub("\\[|\\]","",Vector_Data)
# Replace : or ; with period(.)
Vector_Data <- gsub('[:;]', '.', Vector_Data)
# Remove period(.) if it's in between 2 numbers
Vector_Data <- gsub("[0-9].[0-9]","",Vector_Data)
# Remove numbers
Vector_Data <- gsub('[0-9]+', '', Vector_Data)

# Remove web addresses
Vector_Data <- gsub('http|www\\S+\\s*',"", Vector_Data)
Vector_Data <- gsub(".//","",Vector_Data)

# Remove double white spaces
Vector_Data <- gsub("\\s+", " ", Vector_Data)
Vector_Data <- str_squish(Vector_Data)
Vector_Data <- gsub("\\s\\.", "\\.", Vector_Data)

# Replace consecutive dots by 1 single dot
Vector_Data <- gsub("\\.+",".",Vector_Data)

## ----- Special cleaning that is only used for the readability formulas -----

# Remove single quotation marks because it reduces quality of readability analysis. But it should be kept for lemmatization, so not included in general analysis
Vector_Data_Q <- gsub('[\'’]', '', Vector_Data)

```

## Create quantity measures
Before further processing steps, the data is analyzed on quantity. This includes calculating the number of words, sentences, characters and syllables for every document. 
```{r}
Quantity_Data <- doc_counts(Vector_Data_Q) 
Quantity_Data
```

## Create readability measures
Another exploratory analysis, before further processing, is measuring the readability of the texts
```{r}
Quantity_Data <- readability(Vector_Data_Q) 
Quantity_Data
``` 
## Remove remaining punctuation marks, separators, and symbols; and make text lowercase. 
Remove remaining punctuation marks and make text lowercase. But preserve intra word contractions and intra word dashes (is used for lemmatization)
```{r}
Vector_Data <- removePunctuation(Vector_Data,
                              preserve_intra_word_contractions = TRUE,
                              preserve_intra_word_dashes = TRUE,
                              ucp = FALSE)
Vector_Data <- tolower(Vector_Data)
```

## Tokenizing and lemmatization
The udpipe package is used to tokenize and lemmatize the text. Also, POS tagging is performed. 

The UD_English-ParTUT treebank is utilized, as it is based on various sources, in specific legal texts. It is downloaded and loaded, before analysis can be performed.
Reference Udpipe: (Straka et al. 2016) > Straka Milan, Hajič Jan, Straková Jana. UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia, May 2016. 
```{r}
# Download the model
udpipe_download_model(
  language = "english-partut",
  model_dir = ("~"),
  udpipe_model_repo = "jwijffels/udpipe.models.ud.2.5",
  overwrite = FALSE)
# Load the model
udmodel_english <- udpipe_load_model(file = "~/english-partut-ud-2.5-191206.udpipe")
```

Lemmatize, tokenize and tag Universal Part of Speech (UPOS)
```{r}
x <- udpipe_annotate(udmodel_english, Vector_Data, doc_id = Doc_ID, tokenizer = "tokenizer", tagger = "default", parser = "none")
x <- as.data.frame(x)
x <- x %>% select(doc_id, token_id, lemma, upos)
```

## Remove stopwords
Remove stop words that do not add meaning to the text. First, a stopword list is retrieved. Subsequently, words that equal the stop word are removed from the corpus. NOTE: Remove stop words from the list that contain advisory and restrictive words. 
```{r}
stopwords <- tm::stopwords(kind = "SMART")
# Manually adjust stop words list
x <- x[ ! x$lemma %in% stopwords, ]
```

## Merge multi-word expressions
Merge back multi-word expressions. This is done based on the RAKE algorithm. First, a list needs to be made indicating if a term was a noun, adjective, verb/gerund or not. This list can then be used to filter out the other POS terms. After multi- key words are found, they are taken togehter with the ESG key word list and used to merge tokens. Words are connected through an underscore. For example "corporate governance" becomes "corporate_governance".
```{r}

```

## Filter out tokens based on POS
```{r}

```

## Store the processed data
```{r}
# Quantity Data  

# Tidy text table


```

                                                                                                          