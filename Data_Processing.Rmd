---
title: "Data_Processing"
author: "Susanne_Rehorst"
date: "23-5-2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load or install packages
```{r warning=FALSE}
library(readtext)
library(sylcount)
library(stringr)
library(tm)
library(NLP)
library(udpipe)
library(textclean)
library(dplyr)
```

Set working directory
```{r}
setwd("~/Thesis")
```

# Data Processing
This R Markdown contains the steps to load in the data and applying pre-processing steps, which are required for text analysis

## Loading in Data
For every ESG regulation, the text is loaded in from a .txt file. Subsequently, every regulation is stored as a string value in one cell. Every uploaded document will get an ID, which is used to link the raw text with meta data. First, texts are put into a character string
```{r warning=FALSE}
setwd("~/Thesis/Raw_Data_Text")
Raw_Data <- readtext::readtext("ID*.txt", encoding = "UTF-8")
Vector_Data <- Raw_Data$text
Doc_ID <- c(seq(1, length(Vector_Data), by=1))
```

A separate document with ESG key topics is loaded in
```{r}

```

The meta data is also loaded in. 
```{r}

```

## Merging Data
Mandatory E, S, or G regulations are merged if they come from the same sources and apply to the same region. 
```{r}

```

## Data Cleaning
Preliminary cleaning: Remove double white spaces, double quotes and line break (/n) expressions. This is recommended for the quantity and readability analysis in order to work properly. ... .Also remove alphanumeric characters. 
```{r}
## ----- General cleaning that is used for all analysis -----

# Remove line break expressions: 
Vector_Data <- gsub("\r?\n|\r"," ", Vector_Data)
# Remove double quotation marks, comma's, parentheses, special characters and bullet points
Vector_Data <- gsub('[“”"(),•%$€@/#]', '', Vector_Data)
Vector_Data <- gsub("\\[|\\]","",Vector_Data)
# Replace : or ; with period(.)
Vector_Data <- gsub('[:;]', '.', Vector_Data)
# Remove period(.) if it's in between 2 numbers
Vector_Data <- gsub("[0-9].[0-9]","",Vector_Data)
# Remove numbers
Vector_Data <- gsub('[0-9]+', '', Vector_Data)

# Remove web addresses
Vector_Data <- gsub('http|www\\S+\\s*',"", Vector_Data)
Vector_Data <- gsub(".//","",Vector_Data)

# Remove double white spaces
Vector_Data <- gsub("\\s+", " ", Vector_Data)
Vector_Data <- str_squish(Vector_Data)
Vector_Data <- gsub("\\s\\.", "\\.", Vector_Data)

# Replace consecutive dots by 1 single dot
Vector_Data <- gsub("\\.+",".",Vector_Data)

## ----- Special cleaning that is only used for the readability formulas -----

# Remove single quotation marks because it reduces quality of readability analysis. But it should be kept for lemmatization, so not included in general analysis
Vector_Data_Q <- gsub('[\'’]', '', Vector_Data)

```

## Create quantity and readability measures
Before further processing steps, the data is analyzed on quantity. This includes calculating the number of words, sentences, characters and syllables for every document. Also, several readability measures are calculated.  
```{r}
Quantity_Data <- readability(Vector_Data_Q) 
Quantity_Data
```

## Remove remaining punctuation marks, separators, and symbols; and make text lowercase. 
Remove remaining punctuation marks and make text lowercase. But preserve intra word contractions and intra word dashes (is used for lemmatization).
```{r}
Vector_Data <- removePunctuation(Vector_Data,
                              preserve_intra_word_contractions = TRUE,
                              preserve_intra_word_dashes = TRUE,
                              ucp = FALSE)
Vector_Data <- tolower(Vector_Data)
```

# Measure restrictiveness of texts
Measure in each document the occurence of a pre-selected list of words: 
```{r}
# ---------------- Create list of words that will be used to count restrictiveness  ------------------------ #

# Restrictive words
requirement <- c("requirement","require","required","requires","requiring")
must <- c("must")
need_to <- c("need to","needs to","needing")
have_to <- c("have to","has to","having to")
shall <- c("shall")
prohibtion <- c("prohibit","prohibited","prohibition","prohibiting")
can_not <- c("can not","cannot","do not")
necessary <- c("necessary")
will_not <- c("will not","will")

# Advice words
should <- c("should","should not")
may <- c("may")
ought <- c("ought")
can <- c("can")
advice <- c("advice","advised","advises","advices","advising")

# Action words
keep <- c("keep","keeps","kept","keeping")
indicate <- c("indicate","indicates","indicated","indicating")
do <- c("do","doing","done")
show <- c("show","shows","showing","showed")
use <- c("use","uses","using","used")
report <- c("report","reports","reporting","reported")

# Strongly negative or strongly positive words
negative_strong <- c("never","none","not","nor","nothing","neither")
positive_strong <- c("always","entire","entirely","everywhere","all","everything")

# Exemptions
exemption <- c("except","exemption","exempted","exempt","excluded","excluding")

# Examples
example <- c("example","for instance")

# ----------- Count the occurrence of words and store the results in a data frame ------------------------- #

# set word boundaries so that only a unique match is counted
requirement<- paste0("\\b",requirement,"\\b")
must<- paste0("\\b",must,"\\b")
need_to<- paste0("\\b",need_to,"\\b")
have_to<- paste0("\\b",have_to,"\\b")
shall<- paste0("\\b",shall,"\\b")
prohibtion<- paste0("\\b",prohibtion,"\\b")
can_not<- paste0("\\b",can_not,"\\b")
necessary<- paste0("\\b",necessary,"\\b")
will_not<- paste0("\\b",will_not,"\\b")
should<- paste0("\\b",should,"\\b")
may<- paste0("\\b",may,"\\b")
ought<- paste0("\\b",ought,"\\b")
can<- paste0("\\b",can,"\\b")
advice<- paste0("\\b",advice,"\\b")
keep<- paste0("\\b",keep,"\\b")
indicate<- paste0("\\b",indicate,"\\b")
do<- paste0("\\b",do,"\\b")
show<- paste0("\\b",show,"\\b")
use<- paste0("\\b",use,"\\b")
report<- paste0("\\b",report,"\\b")
negative_strong<- paste0("\\b",negative_strong,"\\b")
positive_strong<- paste0("\\b",positive_strong,"\\b")
exemption<- paste0("\\b",exemption,"\\b")
example<- paste0("\\b",example,"\\b")

# Create a data frame to store the word counts
Restriction_Data <- data.frame()

# Count words
Restriction_Data$requirement<-str_count(Vector_Data,paste(requirement, collapse="|"))
Restriction_Data$must<-str_count(Vector_Data,paste(must, collapse="|"))
Restriction_Data$need_to<-str_count(Vector_Data,paste(need_to, collapse="|"))
Restriction_Data$have_to<-str_count(Vector_Data,paste(have_to, collapse="|"))
Restriction_Data$shall<-str_count(Vector_Data,paste(shall, collapse="|"))
Restriction_Data$prohibtion<-str_count(Vector_Data,paste(prohibtion, collapse="|"))
Restriction_Data$can_not<-str_count(Vector_Data,paste(can_not, collapse="|"))
Restriction_Data$necessary<-str_count(Vector_Data,paste(necessary, collapse="|"))
Restriction_Data$will_not<-str_count(Vector_Data,paste(will_not, collapse="|"))
Restriction_Data$should<-str_count(Vector_Data,paste(should, collapse="|"))
Restriction_Data$may<-str_count(Vector_Data,paste(may, collapse="|"))
Restriction_Data$ought<-str_count(Vector_Data,paste(ought, collapse="|"))
Restriction_Data$can<-str_count(Vector_Data,paste(can, collapse="|"))
Restriction_Data$advice<-str_count(Vector_Data,paste(advice, collapse="|"))
Restriction_Data$keep<-str_count(Vector_Data,paste(keep, collapse="|"))
Restriction_Data$indicate<-str_count(Vector_Data,paste(indicate, collapse="|"))
Restriction_Data$do<-str_count(Vector_Data,paste(do, collapse="|"))
Restriction_Data$show<-str_count(Vector_Data,paste(show, collapse="|"))
Restriction_Data$use<-str_count(Vector_Data,paste(use, collapse="|"))
Restriction_Data$report<-str_count(Vector_Data,paste(report, collapse="|"))
Restriction_Data$negative_strong<-str_count(Vector_Data,paste(negative_strong, collapse="|"))
Restriction_Data$positive_strong<-str_count(Vector_Data,paste(positive_strong, collapse="|"))
Restriction_Data$exemption<-str_count(Vector_Data,paste(exemption, collapse="|"))
Restriction_Data$example<-str_count(Vector_Data,paste(example, collapse="|"))

# Sum up word counts within specific categories      
Restriction_Data$restriction <- sum(requirement,must,need_to,have_to,shall,prohibtion,can_not,necessary,will_not)
Restriction_Data$advice <- sum(should,may,ought,can,advice)
Restriction_Data$action <- sum(keep,indicate,do,show,use,report)

# Create a DOC_ID
Restriction_Data$DOC_ID <- 1:nrow(Restriction_Data)
```


## Tokenizing and lemmatization
The udpipe package is used to tokenize and lemmatize the text. Also, POS tagging is performed. 

The UD_English-ParTUT treebank is utilized, as it is based on various sources, in specific legal texts. It is downloaded and loaded, before analysis can be performed.
Reference Udpipe: (Straka et al. 2016) > Straka Milan, Hajič Jan, Straková Jana. UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia, May 2016. 
```{r}
# Download the model
udpipe_download_model(
  language = "english-partut",
  model_dir = ("~"),
  udpipe_model_repo = "jwijffels/udpipe.models.ud.2.5",
  overwrite = FALSE)
# Load the model
udmodel_english <- udpipe_load_model(file = "~/english-partut-ud-2.5-191206.udpipe")
```

Lemmatize, tokenize and tag Universal Part of Speech (UPOS)
```{r}
x <- udpipe_annotate(udmodel_english, Vector_Data, doc_id = Doc_ID, tokenizer = "tokenizer", tagger = "default", parser = "none")
x <- as.data.frame(x)
x <- x %>% select(doc_id, token_id, lemma, upos)
```

## Remove stopwords
Remove stop words that do not add meaning to the text. First, a stop word list is retrieved. Subsequently, words that equal the stop word are removed from the corpus. 
```{r}
part1 <- c("a", "able", "about", "above", "according", "accordingly", "across", "actually", "after", "afterwards", "again", "against", "ain't", "allows", "almost", "alone", "along", "already", "also", "although", "am", "among", "amongst", "an", "and", "another", "any", "anybody", "anyhow", "anyone", "anything", "anyway", "anyways", "anywhere", "apart", "appear", "appreciate", "appropriate", "are", "aren't", "around", "as", "a's", "aside", "ask", "asking", "associated", "at", "available", "away", "awfully", "b", "be", "became", "because", "become", "becomes", "becoming", "been", "before", "beforehand", "behind", "being", "believe", "below", "beside", "besides", "best", "better", "between", "beyond", "both", "brief", "but", "by", "c", "came", "cause", "causes", "certain", "certainly", "changes", "clearly", "c'mon", "co", "com", "come", "comes", "concerning", "consequently", "consider", "considering", "contain", "containing", "contains", "corresponding", "could", "couldn't", "course", "c's", "currently", "d", "definitely", "described", "despite", "did", "didn't", "different", "does", "doesn't", "done", "don't", "down", "downwards", "during", "e", "each", "edu", "eg", "eight", "either", "else", "elsewhere", "enough", "especially", "et", "etc", "even", "ever", "every", "everybody", "everyone", "ex", "exactly", "f", "far", "few", "fifth", "first", "five", "followed", "following", "follows", "for", "former", "formerly", "forth", "four", "from", "further", "furthermore", "g", "get", "gets", "getting", "given", "gives", "go", "goes", "going", "gone", "got", "gotten", "greetings", "h", "had", "hadn't", "happens", "hardly", "has", "hasn't", "have", "haven't", "having", "he", "hello", "help", "hence", "her", "here", "hereafter", "hereby", "herein", "here's", "hereupon", "hers", "herself", "he's", "hi", "him", "himself", "his", "hither", "hopefully", "how", "howbeit", "however", "i", "i'd", "ie", "if", "ignored", "i'll", "i'm", "immediate", "in", "inasmuch", "inc", "indeed", "indicated", "indicates", "inner", "insofar", "instead", "into", "inward", "is", "isn't", "it", "it'd", "it'll", "its", "it's", "itself", "i've", "j", "just", "k", "keeps", "kept", "know", "known", "knows", "l", "last", "lately", "later", "latter", "latterly", "least", "less", "lest", "let", "let's", "like", "liked", "likely", "little", "look", "looking", "looks", "ltd", "m", "mainly", "many", "maybe", "me", "mean", "meanwhile", "merely", "might", "more", "moreover", "most", "mostly", "much", "my", "myself", "n", "name", "namely", "nd", "near", "nearly", "needs", "nevertheless", "new", "next", "nine", "nobody", "non", "noone", "normally", "novel", "now", "nowhere", "o", "obviously", "of", "off", "often", "oh", "ok", "okay", "old", "on", "once", "one", "ones", "only", "onto", "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "outside", "over", "overall", "own", "p", "particular", "particularly", "per", "perhaps", "placed", "please", "plus", "possible", "presumably", "probably", "provides", "q", "que", "quite", "qv", "r", "rather", "rd", "re", "really", "reasonably", "regarding", "regardless", "regards", "relatively", "respectively", "right", "s", "said", "same", "saw", "say", "saying", "says", "second", "secondly", "see", "seeing", "seem", "seemed", "seeming", "seems", "seen", "self", "selves", "sensible", "sent", "serious", "seriously", "seven", "several", "she", "since", "six", "so", "some", "somebody", "somehow", "someone", "something", "sometime", "sometimes", "somewhat", "somewhere", "soon", "sorry", "specified", "specify", "specifying", "still", "sub", "such", "sup", "sure", "t", "take", "taken", "tell", "tends", "th", "than", "thank", "thanks", "thanx", "that", "thats", "that's", "the", "their", "theirs", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "therefore", "therein", "theres", "there's", "thereupon", "these", "they", "they'd", "they'll", "they're", "they've", "think", "third", "this", "thorough", "thoroughly", "those", "though", "three", "through", "throughout", "thru", "thus")

part2 <- c("to", "together", "too", "took", "toward", "towards", "tried", "tries", "truly", "try", "trying", "t's", "twice", "two", "u", "un", "under", "unfortunately", "unless", "unlikely", "until", "unto", "up", "upon", "us", "used", "useful", "uses", "usually", "uucp", "v", "value", "various", "very", "via", "viz", "vs", "w", "want", "wants", "was", "wasn't", "way", "we", "we'd", "welcome", "well", "we'll", "went", "were", "we're", "weren't", "we've", "what", "whatever", "what's", "when", "whence", "whenever", "where", "whereafter", "whereas", "whereby", "wherein", "where's", "whereupon", "wherever", "whether", "which", "while", "whither", "who", "whoever", "whole", "whom", "who's", "whose", "why", "willing", "wish", "with", "within", "without", "wonder", "would", "would", "wouldn't", "x", "y", "yes", "yet", "you", "you'd", "you'll", "your", "you're", "yours", "yourself", "yourselves", "you've", "z", "zero")

stopwords <- c(part1, part2)
rm(part1)
rm(part2)
```

```{r}
x <- x[ ! x$lemma %in% stopwords, ]
```

## Filter out tokens based on POS
```{r}

```

## Save the processed data
Data is now ready to be processed. This includes creating token vectors that are weighted, for each document in the sample, and calculating similarity. Also, it includes topic analysis (ESG topics) and key word extraction and a regression. 
```{r}
# Quantity Data  

# Restriction Data

# Tidy text format table


```

                                                                                                          